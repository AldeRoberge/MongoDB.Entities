{
  "api/index.html": {
    "href": "api/index.html",
    "title": "Welcome! | MongoDB.Entities",
    "keywords": "Welcome! TODO: Add .NET projects to the src folder and run docfx to generate REAL API Documentation !"
  },
  "index.html": {
    "href": "index.html",
    "title": "Welcome | MongoDB.Entities",
    "keywords": "Welcome This library simplifies access to mongodb by abstracting away the C# mongodb driver and providing some additional features on top of it. The API is clean and intuitive resulting in less lines of code that is more readable/ human friendly than driver code. Advantages Don't have to deal with ObjectIds , BsonDocuments & magic strings unless you want to. Built-in support for One-To-One , One-To-Many and Many-To-Many relationships. Async only API for scalable application development. Query data using LINQ, lambda expressions, filters and aggregation pipelines. Sorting, paging and projecting is made convenient. Simple data migration framework similar to EntityFramework. Programatically define indexes. Full text search (including fuzzy matching) with text indexes. Multi-document transaction support. Multiple database support. Easy bulk operations. Easy Change-stream support. GeoSpatial search. Stream files in chunks to and from mongodb (GridFS alternative). Project types supported: .Net Standard 2.0 (.Net Core 2.0 & .Net Framework 4.6.1 or higher) Get Started >>"
  },
  "wiki/03-Relationships.html": {
    "href": "wiki/03-Relationships.html",
    "title": "Embedded Relationships | MongoDB.Entities",
    "keywords": "Embedded Relationships One-to-one: var author = new Author { Name = \"Eckhart Tolle\" } await author.SaveAsync(); book.Author = author; await book.SaveAsync() as mentioned earlier, calling SaveAsync() persists author to the \"Authors\" collection in the database. it is also stored in book.Author property. so, the author entity now lives in two locations (in the collection and also inside the book entity) and are linked by the ID . if the goal is to embed something as an independant/unlinked document, it is best to use a class that does not inherit from the Entity class or simply use the .ToDocument() method of an entity as explained earlier. Embed Removal: to remove the embedded author , simply do: book.Author = null; await book.SaveAsync(); the original author in the Authors collection is unaffected. Entity Deletion: if you call book.Author.DeleteAsync() , the author entity is deleted from the Authors collection if it was a linked entity. One-to-many: book.OtherAuthors = new Author[] { author1, author2 }; await book.SaveAsync(); Tip: If you are going to store more than a handful of entities within another entity, it is best to store them by reference as described below. Embed Removal: book.OtherAuthors = null; await book.SaveAsync(); the original author1, author2 entities in the Authors collection are unaffected. Entity Deletion: if you call book.OtherAuthors.DeleteAllAsync() the respective author1, author2 entities are deleted from the Authors collection if they were linked entities. Referenced Relationships referenced relationships require a bit of special handling. a one-to-one relationship is defined by using the One<T> class and one-to-many as well as many-to-many relationships are defined by using the Many<T> class. it is also a good idea to initialize the Many<T> properties in the constructor of the parent entity as shown below in order to avoid null-reference exceptions during runtime. public class Book : Entity { public One<Author> MainAuthor { get; set; } public Many<Author> Authors { get; set; } [OwnerSide] public Many<Genre> AllGenres { get; set; } public Book() { this.InitOneToMany(() => Authors); this.InitManyToMany(() => AllGenres, genre => genre.AllBooks); } } notice the parameters of the InitOneToMany and InitManyToMany methods above. the first method only takes one parameter which is just a lambda pointing to the property you want to initialize. the next method takes 2 parameters. first is the property to initialize. second is the property of the other side of the relationship. also note that you specify which side of the relationship a property is by using the attributes [OwnerSide] or [InverseSide] for defining many-to-many relationsips. One-to-one: call the ToReference() method of the entity you want to store as a reference like so: book.MainAuthor = author.ToReference(); await book.SaveAsync(); alternatively you can use the implicit operator functionality by simply assigning an instance or the string ID like so: book.MainAuthor = author; book.MainAuthor = author.ID; Reference Removal: book.MainAuthor = null; await book.SaveAsync(); the original author in the Authors collection is unaffected. Entity Deletion: If you delete an entity that is referenced as above by calling author.DeleteAsync() all references pointing to that entity are automatically deleted. as such, book.MainAuthor.ToEntityAsync() will then return null . the .ToEntityAsync() method is described below. One-to-many & many-to-many: await book.Authors.AddAsync(author); //one-to-many await book.AllGenres.AddAsync(genre); //many-to-many there's no need to call book.SaveAsync() because references are automatically saved using special join collections. you can read more about them in the Schema Changes section. Reference Removal: await book.Authors.RemoveAsync(author); await .AllGenres.RemoveAsync(genre); the original author in the Authors collection is unaffected. also the genre entity in the Genres collection is unaffected. only the relationship between entities are deleted. Entity Deletion: If you delete an entity that is referenced as above by calling author.DeleteAsync() all references pointing to that author entity are automatically deleted. as such, book.Authors will not have author as a child. the same applies to Many-To-Many relationships. deleting any entity that has references pointing to it from other entities results in those references getting deleted and the relationships being invalidated. ToEntityAsync() shortcut: a reference can be turned back in to an entity with the ToEntityAsync() method. var author = await book.MainAuthor.ToEntityAsync(); you can also project the properties you need instead of getting back the complete entity like so: var author = await book.MainAuthor .ToEntityAsync(a => new Author { Name = a.Name, Age = a.Age }); Transaction support adding and removing related entities require passing in the session when used within a transaction. see here for an example. Next Page >>"
  },
  "wiki/04-Queries.html": {
    "href": "wiki/04-Queries.html",
    "title": "LINQ | MongoDB.Entities",
    "keywords": "data can be queried using LINQ, lambda expressions, filters or aggregation pipelines as described below. LINQ see the mongodb c# driver linq documentation to see which LINQ operations are available. also see the c# driver expressions documentation to see all supported expressions. don't forget to first import the mongodb linq extensions with using MongoDB.Driver.Linq; Entity collections: var author = await (from a in DB.Queryable<Author>() where a.Name.Contains(\"Eckhart\") select a).FirstOrDefaultAsync(); Shortcut for entity collections: var authors = from a in author.Queryable() select a; this .Queryable() is an IQueryable for the whole collection of Authors which you can write queries against. Forward Relationship Access: every Many<T> property gives you access to an IQueryable of child entities. var authors = from a in book.Authors.ChildrenQueryable() select a; this .ChildrenQueryable() is an already filtered IQueryable of child entities. For ex: the above .ChildrenQueryable() is limited to only the Authors of that particular Book entity. It does not give you access to all of the Author entities in the Authors collection. Reverse Relationship Access: for example, if you'd like to get all the books belonging to a genre, you can do it with the help of .ParentsQueryable() like so: var books = book.Genres .ParentsQueryable<Book>(\"GenreID\"); you can also pass in an IQueryable of genres and get back an IQueryable of books like shown below: var query = genre.Queryable() .Where(g => g.Name.Contains(\"Music\")); var books = book.Genres .ParentsQueryable<Book>(query); it is basically a convenience method instead of having to do a manual join like the one shown below in order to access parents of one-to-many or many-to-many relationships. Relationship Joins: Many<T>.JoinQueryable() gives you access to all the join records of that particular relationship. A join record has two properties ParentID and ChildID that you can use to gain access to parent Entities like so: var books = from j in book.Authors.JoinQueryable() join b in book.Queryable() on j.ParentID equals b.ID select b; Counting Children you can get how many entities are there in the opposite side of any relationship as shown below: var authorCount = await book.Authors.ChildrenCountAsync(); var bookCount = await author.Books.ChildrenCountAsync(); Find Queries several overloads are available for finding entities as shown below. Find One By ID var author = await DB.Find<Author>().OneAsync(\"ID\"); Find Many By Lambda var authors = await DB.Find<Author>().ManyAsync(a => a.Publisher == \"Harper Collins\"); Find Many By Filter var authors = await DB.Find<Author>() .ManyAsync(f=> f.Eq(a=>a.Surname,\"Stark\") & f.Gt(a=>a.Age,35)); all the filters in the official driver are available for use as shown above. Find By 2D Coordinates var cafes = await DB.Find<Cafe>() .Match(c => c.Location, new Coordinates2D(48.857908, 2.295243), 1000) .ExecuteAsync() see this tutorial for a detailed walkthrough. Find By Aggregation Expression ($expr) var authors = await DB.Find<Author>() .MatchExpression(\"{$gt:['$TotalSales','$SalesGoal']}\") .ExecuteAsync(); aggregation expressions lets you refer to properties from the same entity using the $ notation as shown above. Advanced Find With Sorting, Paging and Projection var authors = await DB.Find<Author>() .Match(a => a.Age > 30) .Sort(a => a.Age, Order.Descending) .Sort(a => a.Name, Order.Ascending) .Skip(1).Limit(1) .Project(a => new Author { Name = a.Name }) .ExecuteAsync(); the search criteria is specified using .Match() which takes either an ID , lambda expression , filter expression , geospatial , or full/fuzzy text search query . sorting is specified using .Sort() which takes in a lambda for the property to sort by and in which order. .Sort() can be used multiple times in order to specify multiple sorting stages. when doing text queries, you can sort the results by mongodb's 'meta text score' by using the .SortByTextScore() method. how many items to skip and take are specified using .Skip() and .Limit() to avoid the complete entity being returned, you can use .Project() with a lambda expression to get back only the properties you need. it is also possible to use projection builder methods like so: .Project(p => p.Include(\"Name\").Exclude(\"Surname\")) to be able to chain projection builder methods like above, please add the import statement using MongoDB.Driver; to your class. it is also possible to specify an exclusion projection with a new expression like so: var res = await DB.Find<Author>() .Match(a => a.ID == \"xxxxxxxxxxx\") .ProjectExcluding(a => new { a.Age, a.Name }) .ExecuteSingleAsync(); doing so will return an Author entity with all the properties populated except for the Age and Name properties. you can also project to a different output type using the generic overload DB.Find<TEntity,TProjection> generic overload. an .Execute*() method is called finally to get back the result of the find command. you can also get a cursor back instead of materialized results by calling .ExecuteCursorAsync() at the end. there are 3 variations of Execute*() you can use. ExecuteAsync() which will return a list of matched entities. ExecuteSingleAsync() which will return only 1 matched entity and will throw an exception if more than 1 entity is matched. ExecuteFirstAsync() which will return the first matched entity. all variations will return a null/default value if nothing was matched. Fluent Aggregation Pipelines 99% of querying requirements can be catered to with the above APIs. in case you need to build fluent aggregation pipelines, use the Fluent method for getting access to the IAggregateFluent<T> interface for a given entity type like so: var author = await DB.Fluent<Author>() .Match(a => a.Surname == \"Stark\" && a.Age > 10) .SortByDescending(a => a.Age) .ThenByAscending(a => a.Name) .Skip(1).Limit(1) .Project(a => new { Test = a.Name }) .SingleOrDefaultAsync(); you'll have to add using MongoDB.Driver; import statement for the async extension methods such as SingleOrDefaultAsync() to work. there are also fluent counterparts of other methods such as: Many<T>.ChildrenFluent() //pre-filtered children of the parent Many<T>.ParentsFluent() //access parents of a given child Many<T>.JoinFluent() //all records of the join collection Transaction.Fluent<T>() //transactional variation of DB.Fluent<T>() DB.FluentTextSearch<T>() //full text search Transaction.FluentTextSearch<T>() //transactional full text search DB.Fluent<T>().MatchExpression() //$expr queries author.Fluent() //shortcut for DB.Fluent<Author>() GeoNear Aggregation Pipelines in order to start a fluent aggregation pipeline with a GeoNear query, simply do the following: var query = DB.FluentGeoNear<Place>( NearCoordinates: new Coordinates2D(48.857908, 2.295243), DistanceField: x => x.DistanceMeters, MaxDistance: 20000); the above code builds an aggregation pipeline that will find all the documents tagged with locations within 20Km from the eiffel tower in paris. you can then add more pipeline stages to the above query in order to do further processing. you can specify all the supported options for $geoNear using the constructor above. Next Page >>"
  },
  "wiki/05-Indexes.html": {
    "href": "wiki/05-Indexes.html",
    "title": "Text Indexes: | MongoDB.Entities",
    "keywords": "use the Index<T> method to define indexes as shown below. specify index keys by chaining calls to the .Key() method. first parameter of the method is a lambda pointing to a property on your entity. second parameter specifies the type of key. finally chain in a call to .CreateAsync() to finish defining the index. TIP: you should define your indexes at the startup of your application so they only run once at launch. alternatively you can define indexes in the static constructor of your entity classes. Text Indexes: await DB.Index<Author>() .Key(a => a.Name, KeyType.Text) .Key(a => a.Surname, KeyType.Text) .CreateAsync(); if the field you want to index is nested within arrays or lists, specify an expression with a [-1] index position like so: .Key(a => a.Books[-1].Reviews[-1].Content, KeyType.Text) in order to index all text properties of an entity, you can create a wildcard text index as follows: .Key(a => a, KeyType.Text) Full Text Search: you can do full text searches after defining a text index as described above with the following: await DB.Find<Book>() .Match(Search.Full, \"search term\") .ExecuteAsync(); you can also start a fluent aggregation pipeline with a $text stage as follows: DB.FluentTextSearch<Book>(Search.Full, \"search term\") click here to see more info on how to do text searches for phrases, negations, any words, etc. Fuzzy Text Search: in order to run a fuzzy text match, simply change the first parameter to Search.Fuzzy as shown here: await DB.Find<Book>() .Match(Search.Fuzzy, \"search term\") .ExecuteAsync(); note: fuzzy text searching requires a bit of special handling, please see here for detailed information. Other Index Types: use the same Index<T> method as above but with the type parameters of the keys set to one of the following: Ascending Descending Geo2D Geo2DSphere Hashed Wildcard Indexing Options: To specify options for index creation, chain in calls to the .Option() method before calling the .Create() method. await DB.Index<Book>() .Key(x => x.Title, KeyType.Descending) .Option(o => o.Background = true) .Option(o => o.Unique = true) .CreateAsync(); Next Page >>"
  },
  "wiki/06-Transactions.html": {
    "href": "wiki/06-Transactions.html",
    "title": "Relationship Manipulation | MongoDB.Entities",
    "keywords": "multi-document transactions are performed as shown below: var book1 = new Book { Title = \"book one\" }; var book2 = new Book { Title = \"book two\" }; await DB.SaveAsync(new[] { book1, book2 }); using (var TN = DB.Transaction()) { var author1 = new Author { Name = \"one\" }; var author2 = new Author { Name = \"two\" }; await TN.SaveAsync(new[] { author1, author2 }); await TN.DeleteAsync<Book>(new[] { book1.ID, book2.ID }); await TN.CommitAsync(); } in the above code, book1 and book2 are saved before the transaction begins. author1 and author2 is created within the transaction and book1 and book2 are deleted within the transaction. a transaction is started when you instantiate a Transaction object either via the factory method DB.Transaction() or new Transaction() . you then perform all transaction logic using the methods supplied by that class such as .SaveAsync() , .DeleteAsync() , .Update() , .Find() instead of the methods supplied by the DB static class like you'd normally do. the methods of the DB class also supports transactions but you would have to supply a session to each method call, which would be less convenient than using the Transaction class. whatever transactional operations you do are only saved to the database once you call the .CommitAsync() method. if you do not call .CommitAsync(), then nothing changes in the database. if an exception occurs before the .CommitAsync() line is reached, all changes are rolled back and the transaction is implicitly terminated. it is best to always wrap the transaction in a using statement because reaching the end of the using statement will automatically end the transaction and dispose the underlying session. if no using statement is used, you will have to manually dispose the transaction object you created in order to finalize things. you can also call .AbortAsync() to abort a transaction prematurely if needed at which point all changes will be rolled back. Relationship Manipulation relationships within a transaction requires passing down the session to the .Add() and .Remove() methods as shown below. using (var TN = DB.Transaction()) { var author = new Author { Name = \"author one\" }; await TN.SaveAsync(author); var book = new Book { Title = \"book one\" }; await TN.SaveAsync(book); await author.Books.AddAsync(book, TN.Session); await author.Books.RemoveAsync(book, TN.Session); await TN.CommitAsync(); } File Storage file storage within a transaction also requires passing down the session like so: using (var TN = DB.Transaction()) { var picture = new Picture { Title = \"my picture\" }; await TN.SaveAsync(picture); var streamTask = new HttpClient() .GetStreamAsync(\"https://placekitten.com/g/4000/4000\"); using (var stream = await streamTask) { await picture.Data.UploadAsync(stream, session: TN.Session); } await TN.CommitAsync(); } Multiple Databases a transaction is always tied to a single database. you can specify which database to use for a transaction in a couple of ways. var TN = DB.Transaction(\"DatabaseName\") // manually specify the database name var TN = DB.Transaction<Book>() // gets the database from the entity type if you try to perform an operation on an entity type that is not connected to the same database as the transaction, mongodb server will throw an exception. NOTE: please read the page on multiple databases to understand how multi-db support works. Next Page >>"
  },
  "wiki/07-Async-Support.html": {
    "href": "wiki/07-Async-Support.html",
    "title": "Next Page >> | MongoDB.Entities",
    "keywords": "This library no longer supports synchronous operations after version 20 as it was discovered that the official mongodb driver is doing faux sync (sync-over-async anti-pattern) under the hood in order to maintain backward compatibility. \"One caveat is that the synchronous legacy API in 2.0 is implemented by calling the low level async API and blocking, waiting for the Task to complete. This is not considered a performant way to use async APIs, so for performance-sensitive code you may prefer to use the 1.10 version of the driver until you are ready to convert your application to use the new async API.\" - Robert Stem stress/load testing showed that it is inefficient at handling large volumes leading to thread-pool starvation. since the official driver has been made fully async after v2.0, it was decided to discourage consumers of this library from using the faux-sync api of the driver by removing all sync wrapper methods and only support async operations for IO bound work going forward. it is highly recommended you build applications that run in server environments fully async from top to bottom in order to make sure they scale well. however, in places where you can't call async code, you can wrap the async methods in a Task.Run() like so: Task.Run(async () => { await DB.InitAsync(\"MyDatabase\", \"127.0.0.1\"); }) .GetAwaiter() .GetResult(); try not to do that except for calling the init method once at app start-up. LINQ Async Extensions in order to write async LINQ queries, make sure to import the mongodb linq extensions and write queries as follows: using MongoDB.Driver; using MongoDB.Driver.Linq; var lastAuthor = await (from a in author.Queryable() orderby a.ModifiedOn descending select a).FirstOrDefaultAsync(); Next Page >>"
  },
  "wiki/08-Schema-Changes.html": {
    "href": "wiki/08-Schema-Changes.html",
    "title": "Reference Collections | MongoDB.Entities",
    "keywords": "be mindful when changing the schema of your entities. the documents/entities stored in mongodb are always overwritten with the current schema/ shape of your entities. for example: Old schema: public class Book : Entity { public int Price { get; set; } } New schema: public class Book : Entity { public int SellingPrice { get; set; } } the data stored in mongodb under Price will be lost upon saving if you do not manually handle the transfer of data from the old property to the new property. Renaming entities: If you for example rename the Book entity to Test ; when you run you app, a new collection called \"Test\" will be created and the old collection called \"Book\" will be orphaned. Any new entities you save will be added to the \"Test\" collection. To avoid that, you can simply rename the collection called \"Book\" to \"Test\" before running your app. Reference Collections Reference(Join) collections use the naming format [Parent~Child(PropertyName)] for One-To-Many and [(PropertyName)Parent~Child(PropertyName)] for Many-To-Many . you don't have to pay any attention to these special collections unless you rename your entities or properties. for ex: if you rename the Book entity to AwesomeBook and property holding it to GoodAuthors just rename the corresponding join collection from [Book~Author(Authors)] to [AwesomeBook~Author(GoodAuthors)] in order to get the references working again. Indexes some care is needed to make sure there won't be any orphaned/ redundant indexes in mongodb after changing your schema. Renaming entities: if you rename an entity, simply rename the corresponding collection in mongodb before running your app as mentioned in the previous section and all indexes will continue to work because indexes are tied to the collections they're in. Changing entity properties or index definitions: after running the app with changed property names or modified index definitions, new indexes will be automatically created to match the current shape of index definitions in your code. you should manually drop indexes that have old schema in order to get rid of redundant/ orphaned indexes. note: the only exception to the above is text indexes. text indexes don't require any manual handling. since there can only be one text index per collection, the library automatically drops and re-creates text indexes when a schema change is detected. Migration System now that you understand how schema changes affect the database, you can automate the needed changes using the newly introduced migration system as explained in the Data Migrations section. Next Page >>"
  },
  "wiki/09-Data-Migrations.html": {
    "href": "wiki/09-Data-Migrations.html",
    "title": "Migration Classes | MongoDB.Entities",
    "keywords": "there's a simple data migration system similar to that of EntityFramework where you can write migration classes with logic for transforming the database and content in order to bring it up-to-date with the current shape of your c# entity schema. Migration Classes create migration classes that has names starting with _number_ followed by anything you'd like and implement the interface IMigration . here are a couple of valid migration class definitions: public class _001_i_will_be_run_first : IMigration { } public class _002_i_will_be_run_second : IMigration { } public class _003_i_will_be_run_third : IMigration { } next implement the UpgradeAsync() method of IMigration and place your migration logic there. Run Migrations in order to execute the migrations, simply call DB.MigrateAsync() whenever you need the database brought up to date. the library keeps track of the last migration run and will execute all newer migrations in the order of their number. in most cases, you'd place the following line of code in the startup of your app right after initializing the database. await DB.MigrateAsync() the above will try to discover all migrations from all assemblies of the application if it's a multi-project solution. you can speed things up a bit by specifying a type so that migrations will only be discovered from the same assembly/project as the specified type, like so: await DB.MigrateAsync<SomeType>(); Examples Merge two properties let's take the scenario of having the first and last names of an Author entity stored in two separate properties and later on deciding to merge them into a single property called \"FullName\". public class _001_merge_first_and_last_name_to_fullname_field : IMigration { private class Author : Entity { public string Name { get; set; } public string Surname { get; set; } public string FullName { get; set; } } public async Task Upgrade() { await DB.Fluent<Author>() .Project(a => new { id = a.ID, fullname = a.Name + \" \" + a.Surname }) .ForEachAsync(async a => { await DB.Update<Author>() .Match(_ => true) .Modify(x => x.FullName, a.fullname) .ExecuteAsync(); }); } } if your collection has many thousands of documents, the above code will be less efficient. below is another more efficient way to achieve the same result using a single mongodb command if your server version is v4.2 or newer. public class _001_merge_first_and_last_name_to_fullname_field : IMigration { public Task Upgrade() { return DB.Update<Author>() .Match(_ => true) .WithPipelineStage(\"{ $set: { FullName: { $concat: ['$Name',' ','$Surname'] } } }\") .ExecutePipelineAsync(); } } Rename a property public class _002_rename_fullname_to_authorname : IMigration { public Task Upgrade() { return DB.Update<Author>() .Match(_ => true) .Modify(b => b.Rename(\"FullName\", \"AuthorName\")) .ExecuteAsync(); } } Rename a collection public class _003_rename_author_collection_to_writer : IMigration { public Task Upgrade() { return DB.Database<Author>() .RenameCollectionAsync(\"Author\", \"Writer\"); } } Next Page >>"
  },
  "wiki/10-Multiple-Databases.html": {
    "href": "wiki/10-Multiple-Databases.html",
    "title": "Usage example: | MongoDB.Entities",
    "keywords": "you can store and retrieve Entities in multiple databases on either a single server or multiple servers. the only requirement is to have unique names for each database. the following example demonstrates how to use multiple databases. Usage example: use the DB.DatabaseFor<T>() method to specify which database you want the Entities of a given type to be stored in. it is only neccessary to do that for the entities you want to store in a non-default database. the default database is the very first database your application initializes. all entities by default are stored in the default database unless specified otherwise using DatabaseFor . as such, the Book entities will be stored in the \"BookShop\" database and the Picture entities are stored in the \"BookShopFILES\" database considering the following code. await DB.InitAsync(\"BookShop\"); await DB.InitAsync(\"BookShopFILES\"); DB.DatabaseFor<Picture>(\"BookShopFILES\"); var book = new Book { Title = \"Power Of Now\" }; await book.SaveAsync(); //alternative: //// await DB.SaveAsync(book); var pic = new Picture { BookID = book.ID, Name = \"Power Of Now Cover Photo\" }; await pic.SaveAsync(); //alternative: //// await DB.SaveAsync(pic); await DB.Update<Picture>() .Match(p => p.ID == pic.ID) .Modify(p => p.Name, \"Updated Cover Photo\") .ExecuteAsync(); var result = await DB.Find<Picture>().OneAsync(pic.ID); NOTE : an entity type is tied to a specific database by calling the DatabaseFor method with the database name on startup. that entity type will always be stored in and retrieved from that specific database only. it is not possible to save a single entity type in multiple databases. Get database name from an entity instance or type var dbName = pic.DatabaseName(); var dbName = DB.DatabaseName<Book>(); the above methods will return the name of the database that the entity is stored in. if not specifically attached to seperate db, it will return the name of the default database. Limitations cross-database relationships with Many<T> is not supported. no cross-database joins/ look-ups as the driver doesn't support it. storing a single entity type in multiple datbases is not supported. Removal of DB instances in v20 if you've used an earlier verion than v20 of this library, you may have used DB instances for performing operations rather than using the static DB class as the entrypoint. In v20, the DB instance support was removed in order to simplify the codebase and optimize performance as that release was a major jump in version with breaking changes to the API. DB instances were a neccessity before v13 in order to support multiple database access. that requirement was removed in v13 after re-architecting the library internals and it was optional to use DB instances up until v20. it can be argued that DB instances are useful for dependency injection but, ideally you should be injecting your repositories (that wrap up the DB methods) into your controllers/ services, not the DB instances directly. if you don't need to unit test or plan to swap persistance technology at a future date, then there's no need to use dependency injection and you are free to do everything via the DB static methods. it is however, recommended that you encapsulate all data access logic in repository or manager classes in order to isolate data persistance logic from your application logic. TIP : as an alternative, have a look at vertical slice architecture as done here for a far superior developer experience compared to the commonly used layerd+di+repositories mess. Next Page >>"
  },
  "wiki/11-Fuzzy-Text-Search.html": {
    "href": "wiki/11-Fuzzy-Text-Search.html",
    "title": "How it works | MongoDB.Entities",
    "keywords": "fuzzy text matching is done using the double metaphone algorythm. with it you can find non-exact matches that sounds similar to your search term. fuzzy matching will only work on properties that are of the type FuzzyString which is supplied by this library. it also requires adding these properties to a text index. here's how you'd typically get the fuzzy search to work: 1. define entity class public class Book : Entity { public FuzzyString AuthorName { get; set; } } 2. create text index await DB.Index<Book>() .Key(b => b.AuthorName, KeyType.Text) .CreateAsync(); 3. store the entity new Book { AuthorName = \"Eckhart Tolle\" }.Save(); 4. do a fuzzy search on the index var results = await DB.Find<Book>() .Match(Search.Fuzzy, \"ekard tole\") .ExecuteAsync(); that's all there's to it... in case you need to start a flunt aggregation pipeline with fuzzy text matching, you can do it like so: DB.FluentTextSearch<Book>(Search.Fuzzy, \"ekard tole\") How it works when you store text using FuzzyString class, the resulting mongodb document will look like this: { ... \"AuthorName\": { \"Value\": \"Eckhart Tolle\", \"Hash\": \"AKRT TL\" } ... } the text is stored in both the original form and also a hash consisting of double metaphone key codes for each word. when you perform a fuzzy search, your search term is converted to double metaphone key codes on the fly and matched against the stored hash to find results using standard mongodb full text functionality. limitations: you are only allowed to store strings of up to 250 characters in length, which is roughly about 25 to 30 words max. Sorting Fuzzy Results: if you'd like to sort the results by relevence (closeness to the original search term) you can use the following utility method: var sortedResults = results.SortByRelevance(\"ekard tole\", b => b.AuthorName); this sorting is done client-side after the fuzzy search retrieves the entities from mongodb. what this extension method does is; it will compare your search term with the value of the property you specify as the second argument to see how close it is using levenshtein distance algorythm. then it will return a new list with the closest matches at the top. you can also exclude items from the resulting list that has a greater edit distance than a given value by specifiying the maxDistance optional parameter like so: var sortedResults = results.SortByRelevance(\"ekard tole\", b => b.AuthorName, 10); Next Page >>"
  },
  "wiki/12-File-Storage.html": {
    "href": "wiki/12-File-Storage.html",
    "title": "Next Page >> | MongoDB.Entities",
    "keywords": "this library features a GridFS alternative where you can stream upload & download files in chunks to keep memory usage at a minimum when dealing with large files. there is no limitation on the size or type of file you can store and the API is designed to be much simpler than GridFS. Create A File Entity inherit from FileEntity abstract class instead of the usual Entity class for defining your file entities like below. You can add any other properties you wish to store with it. public class Picture : FileEntity { public string Title { get; set; } public int Width { get; set; } public int Height { get; set; } } the FileEntity is a sub class of Entity class. so all operations supported by the library can be performed with these file entities. Upload Data before uploading data for a file entity, you must save the file entity first. then simply call the upload method like below by supplying a stream object for it to read the data from: var kitty = new Picture { Title = \"NiceKitty.jpg\", Width = 4000, Height = 4000 }; await kitty.SaveAsync(); var streamTask = new HttpClient() .GetStreamAsync(\"https://placekitten.com/g/4000/4000\"); using (var stream = await streamTask) { await kitty.Data.UploadAsync(stream); } the Data property on the file entity gives you access to a couple of methods for uploading and downloading. with those methods, you can specify upload chunk size , download batch size , operation timeout period , as well as cancellation token for controlling the process. in addition to the properties you added, there will also be FileSize , ChunkCount & UploadSuccessful properties on the file entity. the file size reports how much data has been read from the stream in bytes if the upload is still in progress or the total file size if the upload is complete. chunk count reports how many number of pieces the file has been broken into for storage. UploadSuccessful will only return true if the process completed without any issues. Download Data var picture = await DB.Find<Picture>() .Match(p => p.Title == \"NiceKitty.jpg\") .ExecuteSingleAsync(); using (var stream = File.OpenWrite(\"kitty.jpg\")) { await picture.Data.DownloadAsync(stream); } first retrieve the file entity you want to work with and then call the .Data.DownloadAsync() method by supplying it a stream object to write the data to. alternatively, if the ID of the file entity is known, you can avoid fetching the file entity from the database and access the data directly like so: await DB.File<Picture>(\"xxxxxxxxx\").DownloadAsync(stream); Transaction Support uploading & downloading file data within a transaction requires passing in a session to the upload and download methods. see here for an example. Next Page >>"
  },
  "wiki/13-String-Templates.html": {
    "href": "wiki/13-String-Templates.html",
    "title": "Examples | MongoDB.Entities",
    "keywords": "the mongodb driver has it's limits and sometimes you need to compose queries either by hand or using a query editor/composer. to be able to run those queries and inject values from your C# code, you're required to compose BsonDocuments or do string concatenation which leads to an ugly, unreadable, magic string infested mess. in order to combat this problem and also to couple your C# entity schema to raw queries, this library offers a templating system based on tag replacements. take the following find query for example: db.Book.find( { Title : 'book_name', Price : book_price } ) to couple this text query to your C# models and pass in the values for title and price, you simply surround the parts you want replaced with < and > in order to turn them in to replacement tags/markers like this: db.Book.find( { <Title> : '<book_name>', <Price> : <book_price> } ) the templating system is based on a special class called Template . You simply instantiate a 'Template' object supplying the tagged/marked text query to the constructor. then you chain method calls on the Template object to replace each tag you've marked on the query like so: var query = new Template<Book>(@\" { <Title> : '<book_name>', <Price> : <book_price> }\") .Path(b => b.Title) .Path(b => b.Price) .Tag(\"book_name\",\"The Power Of Now\") .Tag(\"book_price\",\"10.95\"); var result = await DB.Find<Book>() .Match(query) .ExecuteAsync(); the resulting query sent to mongodb is this: db.Book.find( { Title : 'The Power Of Now', Price : 10.95 } ) the .Tag() method simply replaces matching tags on the text query with the supplied value. you don't have to use the < and > characters while using the .Tag() method. infact, avoid it as the tags won't match if you use them. the .Path() method is one of many offered by the Prop class you can use to get the full 'dotted' path of a property by supplying a lambda/member expression. please see the documentation of the 'Prop' class here for the other methods available. notice, that most of these 'Prop' methods only require a single parameter. whatever member expression you supply to them gets converted to a property/field path like this: expression: x => x.Authors[0].Books[0].Title resulting path: Authors.Books.Title if your text query has a tag <Authors.Books.Title> it will get replaced by the resulting path from the 'Prop' class method. the template system will throw an exception in the event of the following 3 scenarios. the input query/text has no tags marked using < and > characters. the input query has tags that you forget to specify replacements for. you have specified replacements that doesn't have a matching tag in the query. this kind of runtime errors are preferable than your code failing silently because the queries didn't produce any results or produced the wrong results. Examples Aggregation Pipeline: var pipeline = new Template<Book>(@\" [ { $match: { <Title>: '<book_name>' } }, { $sort: { <Price>: 1 } }, { $group: { _id: '$<AuthorId>', product: { $first: '$$ROOT' } } }, { $replaceWith: '$product' } ]\") .Path(b => b.Title) .Path(b => b.Price) .Path(b => b.AuthorId) .Tag(\"book_name\", \"MongoDB Templates\"); var book = await DB.PipelineSingleAsync(pipeline); Aggregation Pipeline With Different Result Type: var pipeline = new Template<Book, Author>(@\" [ { $match: { _id: <book_id> } }, { $lookup: { from: '<author_collection>', localField: '<AuthorID>', foreignField: '_id', as: 'authors' } }, { $replaceWith: { $arrayElemAt: ['$authors', 0] } }, { $set: { <Age> : 34 } } ]\" ).Tag(\"book_id\", \"ObjectId('5e572df44467000021005692')\") .Tag(\"author_collection\", DB.Entity<Author>().CollectionName()) .Path(b => b.AuthorID) .PathOfResult(a => a.Age); var authors = await DB.PipelineAsync(pipeline); Find With Match Expression: var query = new Template<Author>(@\" { $and: [ { $gt: [ '$<Age>', <author_age> ] }, { $eq: [ '$<Surname>', '<author_surname>' ] } ] }\") .Path(a => a.Age) .Path(a => a.Surname) .Tag(\"author_age\", \"54\") .Tag(\"author_surname\", \"Tolle\"); var authors = await DB.Find<Author>() .MatchExpression(query) .ExecuteAsync(); Update With Aggregation Pipeline Stages: var pipeline = new Template<Author>(@\" [ { $set: { <FullName>: { $concat: ['$<Name>',' ','$<Surname>'] } } }, { $unset: '<Age>'} ] \") .Path(a => a.FullName) .Path(a => a.Name) .Path(a => a.Surname) .Path(a => a.Age); await DB.Update<Author>() .Match(a => a.ID == \"xxxxx\") .WithPipeline(pipeline) .ExecutePipelineAsync(); Update With Array Filters: var filters = new Template<Author>(@\" [ { '<a.Age>': { $gte: <age> } }, { '<b.Name>': 'Echkart Tolle' } ] \") .Elements(0, author => author.Age) .Elements(1, author => author.Name); .Tag(\"age\", \"55\") var update = new Template<Book>(@\" { $set: { '<Authors.$[a].Age>': <age>, '<Authors.$[b].Name>': '<name>' } } \") .PosFiltered(book => book.Authors[0].Age) .PosFiltered(book => book.Authors[1].Name) .Tag(\"age\", \"55\") .Tag(\"name\", \"Updated Name\"); await DB.Update<Book>() .Match(book => book.ID == \"xxxxxxxx\") .WithArrayFilters(filters) .Modify(update) .ExecuteAsync(); Next Page >>"
  },
  "wiki/14-Change-Streams.html": {
    "href": "wiki/14-Change-Streams.html",
    "title": "1. retrieve a watcher instance | MongoDB.Entities",
    "keywords": "change-stream support is provided via the DB.Watcher<T> registry. you can use a watcher to receive notifications when a given entity type gets either created, updated or deleted. only monitoring at the collection level is supported. 1. retrieve a watcher instance var watcher = DB.Watcher<Author>(\"some-unique-name-for-the-watcher\"); pass a unique string to get a watcher instance. if a watcher by that name already exists in the registry, that instance will be returned. if no such watcher exists, a fresh watcher will be returned. 2. configure and start the watcher watcher.Start( eventTypes: EventType.Created | EventType.Updated | EventType.Deleted, filter: null, batchSize: 25, onlyGetIDs: false, autoResume: true, cancellation: default); OPTIONAL: all except the eventTypes parameter are optional and the default values are shown above. eventTypes: specify what kind of change event you'd like to watch for. multiple types can be specified as shown. filter: if you'd like to receive only a subset of change events, you can do so by supplying a lambda expression to this parameter. for example, if you're interesed in being notified about changes to Authors who are aged 25 and above, set the filter to the following: x => x.FullDocument.Age >= 25 NOTE: filtering cannot be done if the types of change you're interested in includes deletions. because the entity data no longer exists in the database when a deletion occurs and mongodb only returns the entity ID with the change event. batchSize: specify the maximum number of entities you'd like to receive per change notificatin/ a single event firing. the default is 25. onlyGetIDs: set to true if you don't want the complete entity details. in which case all properties except the ID will be null on the received entities. autoResume: change-streams will be auto-resumed by default unless you set this parameter to false. what that means is, say for example you start a watcher and after a while the watcher stops due to an error or an invalidate event . you can then re-start the watcher and it will start receiving notifications from where it left off and you won't lose any changes that occured while the watcher was stopped. if you set this to false, then those changes are skipped and only new changes are received. cancellation: if you'd like to cancel/abort a watcher and close the change-stream permanantly at a future time, pass a cancellation token to this parameter. 3. subscribe to the events onChanges watcher.OnChanges += authors => { foreach (var author in authors) { Console.WriteLine(\"received: \" + author.Name); } }; this event is fired when desired change events have been received from mongodb. for the above example, when author entities have been either created, updated or deleted, this event/action will receive those entities in batches. you can access the received entities via the input action parameter called authors . OnError watcher.OnError += exception => { Console.WriteLine(\"error: \" + exception.Message); if (watcher.CanRestart) { watcher.ReStart(); Console.WriteLine(\"Watching restarted!\"); } }; in case the change-stream ends due to an error, the OnError event will be fired with the exception. you can try to restart the watcher as shown above. OnStop watcher.OnStop += () => { Console.WriteLine(\"Watching stopped!\"); if (watcher.CanRestart) { watcher.ReStart(); Console.WriteLine(\"Watching restarted!\"); } else { Console.WriteLine(\"This watcher is dead!\"); } }; this event will be fired when the internal cursor gets closed due to either you requesting cancellation or an invalidate event occuring such as renaming or dropping of the watched collection. if the cause of stopping is due to aborting via cancellation-token, the watcher has already purged all the event subscribers and no longer can be restarted. if the cause was an invalidate event, you can restart watching as shown above. the existing event subscribers will continue to receive change events. resuming across app restarts you can retrieve a resume token from the ResumeToken property of the watcher like so: var token = watcher.ResumeToken; persist this token to a non-volatile medium and use it upon app startup to initiate a watcher to continue/resume from where the app left off last time it was running. watcher.StartWithToken(token, ...); access all watchers in the registry var watchers = DB.Watchers<Author>(); foreach (var w in watchers) { Console.WriteLine(\"watcher: \" + w.Name); } NOTE: there's a watcher registry per entity type and the watcher names need only be unique to each registry. notes on resource usage each unique watcher instance you create in the registry will consume a thread from the .net thread-pool for iterating the internal change-stream cursor in the background. try to keep the number of watchers in the registry to a minimum due to this reason. NOTE: the threads are not blocked (and released back to the pool) while there are no change events being received as the change-stream cursor is iterated using async/await pattern. but if there's a constant stream of change events being received, these threads will be busy and unavailable to the system. Next Page >>"
  },
  "wiki/15-Extras.html": {
    "href": "wiki/15-Extras.html",
    "title": "The 'Date' Type | MongoDB.Entities",
    "keywords": "The 'Date' Type there's a special Date type you can use to store date/time values in mongodb instead of the regular System.DateTime type. the benefits of using it would be: preserves date/time precision can query using ticks can extend it by inheriting implicitly assignable to and from System.DateTime Examples: // define the entity public class Book : Entity { public Date PublishedOn { get; set; } } // save the entity new Book { PublishedOn = DateTime.UtcNow } .Save(); // query with 'Ticks' var book = await DB.Find<Book>() .Match(b => b.PublishedOn.Ticks < DateTime.UtcNow.Ticks) .ExecuteFirstAsync(); // query with 'DateTime' var book = await DB.Find<Book>() .Match(b => b.PublishedOn.DateTime < DateTime.UtcNow) .ExecuteFirstAsync(); // assign to 'DateTime' from 'Date' DateTime dt = book.PublishedOn; // assign from 'DateTime' to 'Date' Date date = dt; // set/change value with 'Ticks' date.Ticks = DateTime.UtcNow.Ticks; // set/change value with 'DateTime' date.DateTime = DateTime.UtcNow; The 'Prop' Class this static class has several handy methods for getting string property paths from lambda expressions. which can help to eliminate magic strings from your code during advanced scenarios. Prop.Path() returns the full dotted path for a given member expression. Authors[0].Books[0].Title > Authors.Books.Title var path = Prop.Path<Book>(b => b.Authors[0].Books[0].Title); Prop.Property() returns the last property name for a given member expression. Authors[0].Books[0].Title > Title var propName = Prop.Property<Book>(b => b.Authors[0].Books[0].Title); Prop.Collection() returns the collection/entity name for a given entity type. var collectionName = Prop.Collection<Book>(); Prop.PosAll() returns a path with the all positional operator $[] for a given expression. Authors[0].Name > Authors.$[].Name var path = Prop.PosAll<Book>(b => b.Authors[0].Name); Prop.PosFirst() returns a path with the first positional operator $ for a given expression. Authors[0].Name > Authors.$.Name var path = Prop.PosFirst<Book>(b => b.Authors[0].Name); Prop.PosFiltered() returns a path with filtered positional identifiers $[x] for a given expression. Authors[0].Name > Authors.$[a].Name Authors[1].Age > Authors.$[b].Age Authors[2].Books[3].Title > Authors.$[c].Books.$[d].Title index positions start from [0] which is converted to $[a] and so on. var path = Prop.PosFiltered<Book>(b => b.Authors[2].Books[3].Title); Prop.Elements(index, expression) returns a path with the filtered positional identifier prepended to the property path. (0, x => x.Rating) > a.Rating (1, x => x.Rating) > b.Rating index positions start from '0' which is converted to 'a' and so on. var res = Prop.Elements<Book>(0, x => x.Rating); Prop.Elements() returns a path without any filtered positional identifier prepended to it. b => b.Tags > Tags var path = Prop.Elements<Book>(b => b.Tags); Sequential Number Generation we can get mongodb to return a sequentially incrementing number everytime the method .NextSequentialNumber() on an Entity is called. it can be useful when you need to generate custom IDs like in the example below: public class Person : Entity { public string CustomID { get; set; } } var person = new Person(); var number = await person.NextSequentialNumberAsync(); person.CustomID = $\"PID-{number:00000000}-X\"; person.Save(); the value of CustomID would be PID-0000001-X . the next Person entities you create/save would have PID-0000002-X , PID-0000003-X , PID-0000004-X and so on. Alternative Static Method if you don't have an instance of an Entity you can simply call the static method on the DB class like so: var number = await DB.NextSequentialNumberAsync<Person>(); Generation For Any Sequence Name there's also an overload for generating sequential numbers for any given sequence name like so: var number = await DB.NextSequentialNumberAsync(\"SequenceName\"); Considerations keep in mind that there will be a separate sequence of numbers for each Entity type. calling this method issues a single db call in order to increment a counter document in the database and retrieve the number. concurrent access won't result in duplicate numbers being generated but it would cause write locking and performance could suffer. multi db support and async methods with task cancellation support are also available. there is no transaction support in order to avoid number generation unpredictability. however, you can call this method from within a transaction without any trouble."
  },
  "wiki/Entities.html": {
    "href": "wiki/Entities.html",
    "title": "Defining entities | MongoDB.Entities",
    "keywords": "Defining entities add the import statement shown below and create your entities by inheriting the Entity base class. using MongoDB.Entities; public class Book : Entity { public string Title { get; set; } } Ignoring properties if there are some properties on entities you don't want persisted to mongodb, simply use the IgnoreAttribute public class Book : Entity { [Ignore] public string SomeProperty { get; set; } } Customizing collection names by default, mongodb collections will use the names of the entity classes. you can customize the collection names by decorating your entities with the NameAttribute as follows: [Name(\"Writer\")] public class Author : Entity { ... } Optional auto-managed properties there are 2 optional interfaces ICreatedOn & IModifiedOn that you can add to entity class definitions like so: public class Book : Entity, ICreatedOn, IModifiedOn { public string Title { get; set; } public DateTime CreatedOn { get; set; } public DateTime ModifiedOn { get; set; } } if your entity classes implements these interfaces, the library will automatically set the appropriate values so you can use them for sorting operations and other queries."
  },
  "wiki/Entities-Deleting.html": {
    "href": "wiki/Entities-Deleting.html",
    "title": "Deleting entities | MongoDB.Entities",
    "keywords": "Deleting entities deleting entities can be achieved in any of the following ways: await book.DeleteAsync(); await book.OtherAuthors.DeleteAllAsync(); await DB.DeleteAsync<Author>(\"ID\"); await DB.DeleteAsync<Book>(new[] { \"ID1\", \"ID2\" }); await DB.DeleteAsync<Book>(b => b.Title.Contains(\"Trump\"));"
  },
  "wiki/Entities-Saving.html": {
    "href": "wiki/Entities-Saving.html",
    "title": "Saving entities | MongoDB.Entities",
    "keywords": "Saving entities call SaveAsync() on any entity to save the changes. new entities are automatically assigned an ID when they are persisted to the database. var book = new Book { Title = \"The Power Of Now\" }; await book.SaveAsync(); multiple entities can be saved in a single bulk operation like so: var books = new[] { new Book{ Title = \"Book One\" }, new Book{ Title = \"Book Two\" }, new Book{ Title = \"Book Three\"} }; await books.SaveAsync(); alternatively, you can do the following: await DB.SaveAsync(book); await DB.SaveAsync(books); Embedding entities to store an unlinked copy of an entity, call the ToDocument() method. doing so will store an independant duplicate (with a new ID) of the original entity that has no relationship to the original entity. book.Author = author.ToDocument(); book.OtherAuthors = (new Author[] { author2, author3 }).ToDocuments(); await book.SaveAsync();"
  },
  "wiki/Entities-Updating.html": {
    "href": "wiki/Entities-Updating.html",
    "title": "Updating without retrieving | MongoDB.Entities",
    "keywords": "Updating without retrieving you can update a single or batch of entities on the mongodb server by supplying a filter criteria and a subset of properties and the data/ values to be set on them as shown below. await DB.Update<Author>() .Match(a => a.Surname == \"Stark\") .Modify(a => a.Name, \"Brandon\") .Modify(a => a.Surname, \"The Broken\") .ExecuteAsync(); specify the filter criteria with a lambda expression using the .Match() method to indicate which entities/documents you want to target for the update. then use multiples of the .Modify() method to specify which properties you want updated with what data. finally call the .ExecuteAsync() method to run the update command which will take place remotely on the database server. if you'd like to update a single entity, simply target it by .ID like below: await DB.Update<Author>() .MatchID(\"xxxxxxxxxxx\") .Modify(a => a.Surname, \"The Broken\") .ExecuteAsync(); you can also use filters to match entities. all of the filters in the official driver is available for use as follows. await DB.Update<Author>() .Match(f=> f.Eq(a=>a.Surname,\"Stark\") & f.Gt(a=>a.Age,35)) .Modify(a => a.Name, \"Brandon\") .ExecuteAsync(); also you can use all the update definition builder methods supplied by the mongodb driver like so: await DB.Update<Author>() .Match(a => a.ID == \"xxxxxxx\") .Modify(x => x.Inc(a => a.Age, 10)) .Modify(x => x.Set(a => a.Name, \"Brandon\")) .Modify(x => x.CurrentDate(a => a.ModifiedOn)) .ExecuteAsync(); Bulk updates var bulkUpdate = DB.Update<Author>(); bulkUpdate.Match(a => a.Age > 25) .Modify(a => a.Age, 35) .AddToQueue(); bulkUpdate.Match(a => a.Sex == \"Male\") .Modify(a => a.Sex, \"Female\") .AddToQueue(); await bulkUpdate.ExecuteAsync(); first get a reference to a Update<T> class. then specify matching criteria with Match() method and modifications with Modify() method just like you would with a regular update. then instead of calling ExecuteAsync() , simply call AddToQueue() in order to queue it up for batch execution. when you are ready commit the updates, call ExecuteAsync() which will issue a single bulkWrite command to the database. Update and retrieve in order to update an entity and retrieve the updated enity, use the .UpdateAndGet<T>() method on the DB class like so: var result = await DB.UpdateAndGet<Book>() .Match(b => b.ID == \"xxxxxxxxxxxxx\") .Modify(b => b.Title, \"updated title\") .ExecuteAsync(); projection of the returned entity is also possible by using the .Project() method before calling .ExecuteAsync() . Property preserving updates if you'd like to skip one or more properties while saving a complete entity, you can do so with the SavePreservingAsync() method. await book.SavePreservingAsync(x => new { x.Title, x.Price }) this method will build an update command dynamically using reflection and omit the properties you specify. all other properties will be updated in the database with the values from your entity. sometimes, this would be preferable to specifying each and every property with an update command. NOTE: you should only specify root level properties with the New expression. i.e. x => x.Author.Name is not valid. alternatively, you can decorate the properties you want to omit with the [Preserve] attribute and simply call book.SavePreservingAsync() without supplying an expression. if you specify ommissions using both an expression and attributes, the expression will take precedence and the attributes are ignored. you can also do the opposite with the use of [DontPreserve] attribute. if you decorate properties with [DontPreserve] , only the values of those properties are written to the database and all other properties are implicitly ignored when calling SavePreservingAsync() . also, the same rule applies that attributes are ignored if you supply a new expression to SavePreservingAsync() . NOTE: both [DontPreserve] and [Preserve] cannot be used together on the same entity type due to the conflicting nature of what they do. Aggregation pipeline updates starting from mongodb sever v4.2, we can refer to existing fields of the documents when updating as described here . the following example does 3 things. creates a 'FullName' field by concatenating the values from 'FirstName' and 'LastName' fields. creates a 'LowerCaseEmail' field by getting the value from 'Email' field and lower-casing it. removes the Email field. await DB.Update<Author>() .Match(_ => true) .WithPipelineStage(\"{ $set: { FullName: { $concat: ['$FirstName',' ','$LastName'] }}}\") .WithPipelineStage(\"{ $set: { LowerCaseEmail: { $toLower: '$Email' } } }\") .WithPipelineStage(\"{ $unset: 'Email'}\") .ExecutePipelineAsync(); note: pipeline updates and regular updates cannot be used together in one command as it's not supported by the official c# driver. Array filter updates await DB.Update<Book>() .Match(_ => true) .WithArrayFilter(\"{ 'x.Age' : { $gte : 30 } }\") .Modify(\"{ $set : { 'Authors.$[x].Age' : 25 } }\") .ExecuteAsync(); the above update command will set the age of all authors of books where the age is 30 years or older to 25. refer to this document for more info on array filters."
  },
  "wiki/Getting-Started.html": {
    "href": "wiki/Getting-Started.html",
    "title": "Installing | MongoDB.Entities",
    "keywords": "Installing install the nuget package with command: Install-Package MongoDB.Entities Initializing first import the package with using MongoDB.Entities; then initialize the database connection like so: Basic initialization await DB.InitAsync(\"DatabaseName\", \"HostAddress\", PortNumber); Advanced initialization await DB.InitAsync(new MongoClientSettings() { Server = new MongoServerAddress(\"localhost\", 27017), Credential = MongoCredential.CreateCredential(\"DatabaseName\", \"username\", \"password\") }, \"DatabaseName\"); Using a connection string await DB.InitAsync(MongoClientSettings.FromConnectionString( \"mongodb+srv://user:password@cluster.mongodb.net/DatabaseName\"), \"DatabaseName\");"
  }
}